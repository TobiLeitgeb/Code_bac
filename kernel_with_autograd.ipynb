{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import rbf_kernel as rbf_kernel_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel_jax(X1, X2, gamma=1.0, sigma_f=1.0):\n",
    "\n",
    "    sqdist = jnp.sum(X1**2, 1).reshape(-1, 1) + jnp.sum(X2**2, 1) - 2 * jnp.dot(X1, X2.T)\n",
    "    return sigma_f**2 * jnp.exp(-gamma * sqdist)\n",
    "\n",
    "def rbf_kernel_single(x1, x2, gamma):\n",
    "    return jnp.sum(jnp.exp(-gamma * jnp.sum((x1 - x2)**2)))\n",
    "x = np.array([1,2,3]).reshape(-1,1)\n",
    "y = np.array([1,2,3]).reshape(-1,1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now I try to implement the derivatives of the kernel    \n",
    "I want to do this for the helmholtz equation:  \n",
    "\\begin{align}   \n",
    " \\mathcal{L}_x ^\\omega = \\frac{\\partial^2}{\\partial x^2} - \\nu^2 \n",
    "\\end{align}     \n",
    "The first step would be to use differential operator onto the kernel function twice. So that:\n",
    "\\begin{align}\n",
    "K_{new}(x,x') = \\mathcal{L}_x ^\\nu \\mathcal{L}_{x'} ^\\nu K(x,x')\n",
    "\\end{align}   \n",
    "\n",
    "\\begin{align}   \n",
    "    \\mathcal{L}_x ^\\nu \\mathcal{L}_{x'} ^\\nu k(x, x') = \\left(\\frac{\\partial^2}{\\partial x^2} - \\nu^2\\right)\\left(\\frac{\\partial^2}{\\partial x'^2} - \\nu^2\\right)k(x, x') \n",
    "\\end{align} \n",
    "this then results in:\n",
    "\\begin{align*}\n",
    "\\mathcal{L}\\mathcal{L}k(x, x') = & \\frac{\\partial^4}{\\partial x^2 \\partial x'^2} k(x, x') - \\nu^2 \\frac{\\partial^2}{\\partial x^2} k(x, x') - \\nu^2 \\frac{\\partial^2}{\\partial x'^2} k(x, x') + \\nu^4 k(x, x')\n",
    "\\end{align*}\n",
    "Because of the way the rbf works, the two parts with the second derivative can be combined to:\n",
    "\\begin{align*}  \n",
    "\\mathcal{L}\\mathcal{L}k(x, x') = & \\frac{\\partial^4}{\\partial x^2 \\partial x'^2} k(x, x') - 2\\nu^2 \\frac{\\partial^2}{\\partial x^2} k(x, x') + \\nu^4 k(x, x')\n",
    "\\end{align*}\n",
    "To make it more clear, I will call the first part of the result $A$ and the second part $B$, the third part does not need further calculations.\n",
    "\n",
    "In the following I will try to implement this in code.  \n",
    "For A we get:\n",
    "\\begin{align*}\n",
    "A_{ij} = (16{\\gamma}^4\\cdot\\left(x_i-y_j\\right)^4-48{\\gamma}^3\\cdot\\left(x_i-y_j\\right)^2+12{\\gamma}^2)\\mathrm{e}^{-{\\gamma}\\cdot\\left(x_i-y_j\\right)^2}\n",
    "\\end{align*}\n",
    "For B we get:\n",
    "\\begin{align*}\n",
    "B_{ij} = (4{\\gamma}^2\\cdot\\left(x_i-y_j\\right)^2-2{\\gamma})\\mathrm{e}^{-{\\gamma}\\cdot\\left(x_i-y_j\\right)^2}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "A has to computed elementwise just like the kernel itself. I will now try to do this once with the expressions and once with autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_scratch(x,x_bar, hyperparameters):\n",
    "    gamma, sigma_f = hyperparameters[0], hyperparameters[1]\n",
    "    \n",
    "    kernel_values = rbf_kernel_jax(x, x_bar, gamma,sigma_f)\n",
    "    #kernel_values = rbf_kernel_jax(x, x_bar, l= 1/np.sqrt(2*gamma), sigma_f_sq = sigma_f**2)\n",
    "\n",
    "    n, m = x.shape[0], x_bar.shape[0]\n",
    "    dk_ff = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            dist_sq = (x[i] - x_bar[j])**2\n",
    "            factor = 16*gamma**4*dist_sq**2 - 48*gamma**3*dist_sq + 12*gamma**2\n",
    "            dk_ff[i, j] = factor\n",
    "    return dk_ff * kernel_values\n",
    "def B_scratch(x,x_bar, hyperparameters):\n",
    "    gamma, sigma_f = hyperparameters[0], hyperparameters[1]\n",
    "    nu = hyperparameters[2]\n",
    "    kernel_values = sigma_f**2*rbf_kernel_sklearn(x, x_bar, gamma)\n",
    "\n",
    "    n, m = x.shape[0], x_bar.shape[0]\n",
    "    dk_ff = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            dist_sq = (x[i] - x_bar[j])**2\n",
    "            factor = -2*nu**2*(4*gamma**2 * dist_sq - 2*gamma)\n",
    "            dk_ff[i, j] = factor*kernel_values[i,j]\n",
    "    return dk_ff\n",
    "def C_scratch(x,x_bar, hyperparameters):\n",
    "    gamma, sigma_f = hyperparameters[0], hyperparameters[1]\n",
    "    nu = hyperparameters[2]\n",
    "    kernel_values = sigma_f**2*rbf_kernel_sklearn(x, x_bar, gamma)\n",
    "\n",
    "    return nu**4*kernel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap, jit\n",
    "from functools import partial\n",
    "x = np.array([1.0,2.0,3.0])\n",
    "y = np.array([1.0,2.0,3.0])\n",
    "gamma = 1\n",
    "@jit\n",
    "def rbf_kernel_single(x1, x2, params):\n",
    "    gamma, sigma_f = params[0], params[1]\n",
    "    return sigma_f**2*jnp.exp(-gamma * jnp.sum((x1 - x2)**2))\n",
    "\n",
    "#done with help of https://jejjohnson.github.io/research_notebook/content/notes/kernels/kernel_derivatives.html\n",
    "\n",
    "def A_autograd(x,x_bar, hyperparameters):\n",
    "    \"\"\"only works for 1D arrays atm \"\"\"\n",
    "\n",
    "    # Vectorize the kernel function, in_axes specifies which argument is vectorized. I could also use the lambda function for the hyperparameter argument, but this is more readable.\n",
    "    params_rbf = hyperparameters[:2]\n",
    "    # first_vmap = vmap(rbf_kernel_single, in_axes=(None, 0, None))\n",
    "    # vectorized_rbf_kernel = vmap(first_vmap, in_axes=(0, None, None))\n",
    "\n",
    "    # # Now rbf_kernel should accept 2D arrays for x1 and x2, however atm it only works for 1D arrays\n",
    "    # K =  vectorized_rbf_kernel(x, x_bar, params_rbf)\n",
    "\n",
    "    # Now compute the derivatives\n",
    "    second_derivative_x = grad(grad(rbf_kernel_single, argnums=0), argnums=0)\n",
    "    fourth_derivative = grad(grad(second_derivative_x, argnums=1), argnums=1)\n",
    "\n",
    "    # Vectorize the derivative function \n",
    "    fourth_derivative_vectorized = vmap(vmap(fourth_derivative, in_axes=(None, 0, None)), in_axes=(0, None, None))\n",
    "\n",
    "    # Now fourth_derivative_vectorized will accept 2D arrays for x1 and x2\n",
    "    K_4th_derivative = fourth_derivative_vectorized(x, x_bar, params_rbf)\n",
    "    return K_4th_derivative \n",
    "\n",
    "\n",
    "def A_autograd_2(x,x_bar, hyperparameters):\n",
    "    \"\"\" to check if the autograd function works, however this is very slow in comparison to the other function \"\"\"\n",
    "    params_rbf = hyperparameters[:2]\n",
    "    \n",
    "    n, m = x.shape[0], x_bar.shape[0]\n",
    "    dk_ff = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            \n",
    "            dk_ff[i, j] = grad(grad(grad(grad(rbf_kernel_single, argnums=1), argnums=1), argnums=0), argnums=0)(x[i], x_bar[j], params_rbf)\n",
    "    return dk_ff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 192.         27.2053   -118.562744  -43.30729 ]\n",
      " [  27.2053    192.         27.2053   -118.56274 ]\n",
      " [-118.562744   27.2053    192.00003    27.205303]\n",
      " [ -43.30729  -118.56274    27.205303  192.      ]]\n",
      "[[ 192.         27.205296 -118.56275   -43.307293]\n",
      " [  27.205296  192.         27.205296 -118.56276 ]\n",
      " [-118.56275    27.205296  192.         27.205307]\n",
      " [ -43.307293 -118.56276    27.205307  192.      ]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = [2,2,4]\n",
    "x = np.linspace(0,1,4).reshape(-1,1)\n",
    "y = np.linspace(0,1,4).reshape(-1,1)\n",
    "print(A_scratch(x,y,hyperparameters))\n",
    "\n",
    "x_1 = np.linspace(0,1,4)\n",
    "y_1 = np.linspace(0,1,4)\n",
    "#print(A_autograd_2(x,y,hyperparameters))\n",
    "print(A_autograd(x_1,y_1,hyperparameters))\n",
    "print(np.allclose(A_autograd(x_1,y_1,hyperparameters),A_scratch(x,y,hyperparameters)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Both are the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B - comparison\n",
      "[[ 576.        400.888      41.035553 -211.89856 ]\n",
      " [ 400.888     576.        400.888      41.035583]\n",
      " [  41.035553  400.888     576.        400.88806 ]\n",
      " [-211.89856    41.035583  400.88806   576.      ]]\n",
      "[[ 576.          400.88801393   41.03554486 -211.89855811]\n",
      " [ 400.88801393  576.          400.88801393   41.03554486]\n",
      " [  41.03554486  400.88801393  576.          400.88801393]\n",
      " [-211.89855811   41.03554486  400.88801393  576.        ]]\n",
      "True B \n",
      "\n",
      "C - comparison\n",
      "[[2304.      2061.7097  1477.2797   847.59424]\n",
      " [2061.7097  2304.      2061.7097  1477.2797 ]\n",
      " [1477.2797  2061.7097  2304.      2061.71   ]\n",
      " [ 847.59424 1477.2797  2061.71    2304.     ]]\n",
      "[[2304.         2061.70978594 1477.27961494  847.59423246]\n",
      " [2061.70978594 2304.         2061.70978594 1477.27961494]\n",
      " [1477.27961494 2061.70978594 2304.         2061.70978594]\n",
      " [ 847.59423246 1477.27961494 2061.70978594 2304.        ]]\n",
      "True C\n"
     ]
    }
   ],
   "source": [
    "def B_autograd(x,x_bar, hyperparameters):\n",
    "    params_rbf = hyperparameters[:2]\n",
    "    nu = hyperparameters[2]\n",
    "    first_vmap = vmap(rbf_kernel_single, in_axes=(None, 0, None))\n",
    "    vectorized_rbf_kernel = vmap(first_vmap, in_axes=(0, None, None))\n",
    "\n",
    "    # here we only need the second derivative once with respect to x2\n",
    "    second_derivative_x2 = grad(grad(rbf_kernel_single, argnums=1), argnums=1)\n",
    "    second_derivative_x2_vectorized = vmap(vmap(second_derivative_x2, in_axes=(None, 0, None)), in_axes=(0, None, None))\n",
    "    \n",
    "    return -2*nu**2*second_derivative_x2_vectorized(x, x_bar, params_rbf)\n",
    "def C_vectorized(x,x_bar,hyperparameters):\n",
    "    params_rbf = hyperparameters[:2]\n",
    "    nu = hyperparameters[2]\n",
    "    first_vmap = vmap(rbf_kernel_single, in_axes=(None, 0, None))\n",
    "    vectorized_rbf_kernel = vmap(first_vmap, in_axes=(0, None, None))\n",
    "    return nu**4*vectorized_rbf_kernel(x, x_bar, params_rbf)\n",
    "#compare with the scratch function\n",
    "hyperparameters = [1,3,4]\n",
    "print(\"B - comparison\")\n",
    "print(B_autograd(x_1,y_1,hyperparameters))\n",
    "print(B_scratch(x,y,hyperparameters))\n",
    "print(np.allclose(B_autograd(x_1,y_1,hyperparameters),B_scratch(x,y,hyperparameters)), \"B\",\"\\n\")\n",
    "print(\"C - comparison\")\n",
    "print(C_vectorized(x_1,y_1,hyperparameters))\n",
    "print(C_scratch(x,y,hyperparameters))\n",
    "print(np.allclose(C_vectorized(x_1,y_1,hyperparameters),C_scratch(x,y,hyperparameters)),\"C\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the same result. So the dimplementation shold be correct.     \n",
    "Now lets try to implement the kernel with the derivatives.  \n",
    "At first I want to compute the marginal log likelihood to optimize the hyperparameters $\\gamma, \\sigma_f$ and the infered hyperparameter from the Helmholtzequation $\\nu$.      \n",
    "The marginal log likelihood is given by:\n",
    "\\begin{align*}\n",
    "\\log p(\\mathbf{y} \\mid \\mathbf{X}, \\gamma, \\sigma_f, \\nu) = & -\\frac{1}{2}\\mathbf{y}^T\\left(K_{new} + \\sigma_n^2\\mathbf{I}\\right)^{-1}\\mathbf{y} - \\frac{1}{2}\\log\\left|K_{new} + \\sigma_n^2\\mathbf{I}\\right| - \\frac{n}{2}\\log 2\\pi\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_derivative_matrix_scratch(x_train, x_test,noise, hyperparameters):\n",
    "    \"\"\" create the derivative matrix with the three functions above \"\"\"\n",
    "    A = A_scratch(x_train, x_test, hyperparameters)\n",
    "    B = B_scratch(x_train, x_test, hyperparameters)\n",
    "    C = C_scratch(x_train, x_test, hyperparameters)\n",
    "    return A + B + C + noise * np.eye(len(x_train))\n",
    "\n",
    "def create_derivative_matrix_jax(x_train, x_test,noise, hyperparameters):\n",
    "    \"\"\" create the derivative matrix with the three functions above \"\"\"\n",
    "    A = A_autograd(x_train, x_test, hyperparameters)\n",
    "    B = B_autograd(x_train, x_test, hyperparameters)\n",
    "    C = C_vectorized(x_train, x_test, hyperparameters)\n",
    "    return A + B + C + noise * np.eye(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.3 ms ± 2.57 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "size = 10\n",
    "x_1, y_1 = np.linspace(0,1,size), np.linspace(0,1,size)\n",
    "hyperparameters = [1,3,4]\n",
    "x,y = np.linspace(0,1,size).reshape(-1,1), np.linspace(0,1,size).reshape(-1,1)\n",
    "\n",
    "%timeit create_derivative_matrix_jax(x_1,y_1,0.001,hyperparameters)\n",
    "#%timeit create_derivative_matrix_scratch(x,y,0.001, hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
